Creating Directory SPARK_WORKER_DIR /global/cscratch1/sd/qpzhang/spark/1383367
Creating /global/cscratch1/sd/qpzhang/spark/1383367/slaves file
Determining the master node name...
Master node is nid00952
16/03/16 13:32:26 INFO SparkContext: Running Spark version 1.5.1
16/03/16 13:32:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/03/16 13:32:26 INFO SecurityManager: Changing view acls to: qpzhang
16/03/16 13:32:26 INFO SecurityManager: Changing modify acls to: qpzhang
16/03/16 13:32:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(qpzhang); users with modify permissions: Set(qpzhang)
16/03/16 13:32:27 INFO Slf4jLogger: Slf4jLogger started
16/03/16 13:32:27 INFO Remoting: Starting remoting
16/03/16 13:32:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.128.3.191:34504]
16/03/16 13:32:27 INFO Utils: Successfully started service 'sparkDriver' on port 34504.
16/03/16 13:32:27 INFO SparkEnv: Registering MapOutputTracker
16/03/16 13:32:27 INFO SparkEnv: Registering BlockManagerMaster
16/03/16 13:32:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-44cfc473-20d5-4dcc-864f-233bfe891eb9
16/03/16 13:32:27 INFO DiskBlockManager: Created local directory at /global/cscratch1/sd/qpzhang/blockmgr-846891a8-9fc4-4c0a-bf15-bea14d6c2cd1
16/03/16 13:32:27 INFO MemoryStore: MemoryStore started with capacity 7.8 GB
16/03/16 13:32:28 INFO HttpFileServer: HTTP File server directory is /tmp/spark-7d6c4fab-fd1e-4a8f-9628-9e2678c581d9/httpd-98eaddb6-35ff-4bc7-91fd-8e7152351053
16/03/16 13:32:28 INFO HttpServer: Starting HTTP Server
16/03/16 13:32:28 INFO Utils: Successfully started service 'HTTP file server' on port 45522.
16/03/16 13:32:28 INFO SparkEnv: Registering OutputCommitCoordinator
16/03/16 13:32:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/03/16 13:32:28 INFO SparkUI: Started SparkUI at http://10.128.3.191:4040
16/03/16 13:32:28 INFO SparkContext: Added JAR file:/global/homes/q/qpzhang/Dropbox/NewBitbucket_for_GeneLearn/jgi-genelearn/scripts/Spark/Training/target/scala-2.10/simple-project_2.10-1.0.jar at http://10.128.3.191:45522/jars/simple-project_2.10-1.0.jar with timestamp 1458160348474
16/03/16 13:32:28 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Connecting to master spark://nid00952:7077...
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160316133228-0000
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/0 on worker-20160316133225-10.128.6.35-43154 (10.128.6.35:43154) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/0 on hostPort 10.128.6.35:43154 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/1 on worker-20160316133225-10.128.6.36-49174 (10.128.6.36:49174) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/1 on hostPort 10.128.6.36:49174 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/2 on worker-20160316133225-10.128.6.33-45530 (10.128.6.33:45530) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/2 on hostPort 10.128.6.33:45530 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/3 on worker-20160316133225-10.128.8.187-39987 (10.128.8.187:39987) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/3 on hostPort 10.128.8.187:39987 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/4 on worker-20160316133225-10.128.4.112-37397 (10.128.4.112:37397) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/4 on hostPort 10.128.4.112:37397 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/5 on worker-20160316133225-10.128.6.34-43606 (10.128.6.34:43606) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/5 on hostPort 10.128.6.34:43606 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/6 on worker-20160316133225-10.128.5.7-55802 (10.128.5.7:55802) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/6 on hostPort 10.128.5.7:55802 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/7 on worker-20160316133225-10.128.8.188-41533 (10.128.8.188:41533) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/7 on hostPort 10.128.8.188:41533 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor added: app-20160316133228-0000/8 on worker-20160316133225-10.128.8.63-42360 (10.128.8.63:42360) with 64 cores
16/03/16 13:32:28 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160316133228-0000/8 on hostPort 10.128.8.63:42360 with 64 cores, 32.0 GB RAM
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/0 is now RUNNING
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/1 is now RUNNING
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/2 is now RUNNING
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/3 is now RUNNING
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/4 is now RUNNING
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/5 is now RUNNING
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/6 is now RUNNING
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/7 is now RUNNING
16/03/16 13:32:28 INFO AppClient$ClientEndpoint: Executor updated: app-20160316133228-0000/8 is now RUNNING
16/03/16 13:32:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44567.
16/03/16 13:32:29 INFO NettyBlockTransferService: Server created on 44567
16/03/16 13:32:29 INFO BlockManagerMaster: Trying to register BlockManager
16/03/16 13:32:29 INFO BlockManagerMasterEndpoint: Registering block manager 10.128.3.191:44567 with 7.8 GB RAM, BlockManagerId(driver, 10.128.3.191, 44567)
16/03/16 13:32:29 INFO BlockManagerMaster: Registered BlockManager
16/03/16 13:32:29 ERROR SparkContext: Error initializing SparkContext.
java.io.FileNotFoundException: File file:/global/cscratch1/sd/qpzhang/spark/spark_event_log does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:100)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:541)
	at Simple$.main(SimpleApp.scala:28)
	at Simple.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/03/16 13:32:29 INFO SparkUI: Stopped Spark web UI at http://10.128.3.191:4040
16/03/16 13:32:29 INFO DAGScheduler: Stopping DAGScheduler
16/03/16 13:32:29 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/03/16 13:32:29 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/03/16 13:32:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/03/16 13:32:29 INFO MemoryStore: MemoryStore cleared
16/03/16 13:32:29 INFO BlockManager: BlockManager stopped
16/03/16 13:32:29 INFO BlockManagerMaster: BlockManagerMaster stopped
16/03/16 13:32:29 INFO SparkContext: Successfully stopped SparkContext
16/03/16 13:32:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
Exception in thread "main" java.io.FileNotFoundException: File file:/global/cscratch1/sd/qpzhang/spark/spark_event_log does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:100)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:541)
	at Simple$.main(SimpleApp.scala:28)
	at Simple.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/03/16 13:32:29 INFO ShutdownHookManager: Shutdown hook called
16/03/16 13:32:29 INFO ShutdownHookManager: Deleting directory /global/cscratch1/sd/qpzhang/spark-ddeca197-8426-4bc2-b7b6-089232b03138
16/03/16 13:32:29 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/03/16 13:32:29 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/03/16 13:32:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-7d6c4fab-fd1e-4a8f-9628-9e2678c581d9
