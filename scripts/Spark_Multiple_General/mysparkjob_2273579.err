Creating Directory SPARK_WORKER_DIR /global/cscratch1/sd/qpzhang/spark/2273579
Creating /global/cscratch1/sd/qpzhang/spark/2273579/slaves file
Determining the master node name...
Master node is nid00422
16/05/20 17:44:08 INFO SparkContext: Running Spark version 1.5.1
16/05/20 17:44:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/05/20 17:44:08 INFO SecurityManager: Changing view acls to: qpzhang
16/05/20 17:44:08 INFO SecurityManager: Changing modify acls to: qpzhang
16/05/20 17:44:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(qpzhang); users with modify permissions: Set(qpzhang)
16/05/20 17:44:09 INFO Slf4jLogger: Slf4jLogger started
16/05/20 17:44:09 INFO Remoting: Starting remoting
16/05/20 17:44:09 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.128.1.169:53118]
16/05/20 17:44:09 INFO Utils: Successfully started service 'sparkDriver' on port 53118.
16/05/20 17:44:09 INFO SparkEnv: Registering MapOutputTracker
16/05/20 17:44:09 INFO SparkEnv: Registering BlockManagerMaster
16/05/20 17:44:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b71b4236-973b-43b9-9bb2-5bf5e724257a
16/05/20 17:44:09 INFO DiskBlockManager: Created local directory at /global/cscratch1/sd/qpzhang/blockmgr-85c38648-9f76-4f12-a30f-f0827487c534
16/05/20 17:44:09 INFO MemoryStore: MemoryStore started with capacity 25.9 GB
16/05/20 17:44:09 INFO HttpFileServer: HTTP File server directory is /tmp/spark-971500e0-0485-4262-a714-f4d0e6c60d9d/httpd-9d1aa84f-dccb-4b4f-babb-fc4e9aae3ad3
16/05/20 17:44:09 INFO HttpServer: Starting HTTP Server
16/05/20 17:44:09 INFO Utils: Successfully started service 'HTTP file server' on port 40151.
16/05/20 17:44:09 INFO SparkEnv: Registering OutputCommitCoordinator
16/05/20 17:44:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/05/20 17:44:10 INFO SparkUI: Started SparkUI at http://10.128.1.169:4040
16/05/20 17:44:10 INFO SparkContext: Added JAR file:/global/u2/q/qpzhang/Dropbox/Bitbucket/jgi-genelearn/scripts/Spark_Multiple_General/target/scala-2.10/simple-project_2.10-1.0.jar at http://10.128.1.169:40151/jars/simple-project_2.10-1.0.jar with timestamp 1463791450072
16/05/20 17:44:10 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Connecting to master spark://nid00422:7077...
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160520174410-0000
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/0 on worker-20160520174406-10.128.3.129-51720 (10.128.3.129:51720) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/0 on hostPort 10.128.3.129:51720 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/1 on worker-20160520174407-10.128.3.131-45150 (10.128.3.131:45150) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/1 on hostPort 10.128.3.131:45150 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/2 on worker-20160520174405-10.128.1.172-38502 (10.128.1.172:38502) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/2 on hostPort 10.128.1.172:38502 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/3 on worker-20160520174405-10.128.3.157-55157 (10.128.3.157:55157) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/3 on hostPort 10.128.3.157:55157 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/4 on worker-20160520174407-10.128.3.133-43001 (10.128.3.133:43001) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/4 on hostPort 10.128.3.133:43001 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/5 on worker-20160520174406-10.128.3.134-58065 (10.128.3.134:58065) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/5 on hostPort 10.128.3.134:58065 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/6 on worker-20160520174408-10.128.2.214-60993 (10.128.2.214:60993) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/6 on hostPort 10.128.2.214:60993 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/7 on worker-20160520174408-10.128.3.172-51294 (10.128.3.172:51294) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/7 on hostPort 10.128.3.172:51294 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/8 on worker-20160520174409-10.128.8.50-37886 (10.128.8.50:37886) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/8 on hostPort 10.128.8.50:37886 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/9 on worker-20160520174408-10.128.3.30-34494 (10.128.3.30:34494) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/9 on hostPort 10.128.3.30:34494 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/10 on worker-20160520174409-10.128.3.31-47822 (10.128.3.31:47822) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/10 on hostPort 10.128.3.31:47822 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/11 on worker-20160520174407-10.128.2.253-42296 (10.128.2.253:42296) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/11 on hostPort 10.128.2.253:42296 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/12 on worker-20160520174406-10.128.3.159-57078 (10.128.3.159:57078) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/12 on hostPort 10.128.3.159:57078 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/13 on worker-20160520174408-10.128.8.53-42866 (10.128.8.53:42866) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/13 on hostPort 10.128.8.53:42866 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/14 on worker-20160520174405-10.128.3.158-47444 (10.128.3.158:47444) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/14 on hostPort 10.128.3.158:47444 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/15 on worker-20160520174407-10.128.3.3-58229 (10.128.3.3:58229) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/15 on hostPort 10.128.3.3:58229 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/16 on worker-20160520174408-10.128.3.173-46454 (10.128.3.173:46454) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/16 on hostPort 10.128.3.173:46454 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/17 on worker-20160520174407-10.128.3.4-51801 (10.128.3.4:51801) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/17 on hostPort 10.128.3.4:51801 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/18 on worker-20160520174406-10.128.1.170-59478 (10.128.1.170:59478) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/18 on hostPort 10.128.1.170:59478 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/19 on worker-20160520174405-10.128.3.128-33498 (10.128.3.128:33498) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/19 on hostPort 10.128.3.128:33498 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/20 on worker-20160520174406-10.128.3.132-58834 (10.128.3.132:58834) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/20 on hostPort 10.128.3.132:58834 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/21 on worker-20160520174407-10.128.3.2-39385 (10.128.3.2:39385) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/21 on hostPort 10.128.3.2:39385 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/22 on worker-20160520174406-10.128.3.155-35087 (10.128.3.155:35087) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/22 on hostPort 10.128.3.155:35087 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/23 on worker-20160520174406-10.128.3.156-40445 (10.128.3.156:40445) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/23 on hostPort 10.128.3.156:40445 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/24 on worker-20160520174406-10.128.3.130-40377 (10.128.3.130:40377) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/24 on hostPort 10.128.3.130:40377 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/25 on worker-20160520174406-10.128.2.215-52932 (10.128.2.215:52932) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/25 on hostPort 10.128.2.215:52932 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/26 on worker-20160520174408-10.128.3.175-53942 (10.128.3.175:53942) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/26 on hostPort 10.128.3.175:53942 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/27 on worker-20160520174407-10.128.2.254-42808 (10.128.2.254:42808) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/27 on hostPort 10.128.2.254:42808 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/28 on worker-20160520174408-10.128.3.174-51776 (10.128.3.174:51776) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/28 on hostPort 10.128.3.174:51776 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/29 on worker-20160520174408-10.128.3.183-45609 (10.128.3.183:45609) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/29 on hostPort 10.128.3.183:45609 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/30 on worker-20160520174405-10.128.2.252-50069 (10.128.2.252:50069) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/30 on hostPort 10.128.2.252:50069 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/31 on worker-20160520174406-10.128.1.171-58369 (10.128.1.171:58369) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/31 on hostPort 10.128.1.171:58369 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/32 on worker-20160520174409-10.128.8.51-48659 (10.128.8.51:48659) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/32 on hostPort 10.128.8.51:48659 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/0 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/1 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/2 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/3 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/4 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/5 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/6 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/7 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/8 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/9 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/10 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/11 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/12 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/13 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/14 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/15 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/16 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/17 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/18 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/19 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/20 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/21 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/22 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/23 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/24 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/25 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/26 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/27 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/28 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/29 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/30 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/31 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/32 is now RUNNING
16/05/20 17:44:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39209.
16/05/20 17:44:10 INFO NettyBlockTransferService: Server created on 39209
16/05/20 17:44:10 INFO BlockManagerMaster: Trying to register BlockManager
16/05/20 17:44:10 INFO BlockManagerMasterEndpoint: Registering block manager 10.128.1.169:39209 with 25.9 GB RAM, BlockManagerId(driver, 10.128.1.169, 39209)
16/05/20 17:44:10 INFO BlockManagerMaster: Registered BlockManager
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/33 on worker-20160520174409-10.128.8.52-40983 (10.128.8.52:40983) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/33 on hostPort 10.128.8.52:40983 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/34 on worker-20160520174409-10.128.3.182-47459 (10.128.3.182:47459) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/34 on hostPort 10.128.3.182:47459 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/33 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/34 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/35 on worker-20160520174409-10.128.4.183-38133 (10.128.4.183:38133) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/35 on hostPort 10.128.4.183:38133 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/35 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/25 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/36 on worker-20160520174409-10.128.3.180-52643 (10.128.3.180:52643) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/36 on hostPort 10.128.3.180:52643 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/2 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/30 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/37 on worker-20160520174409-10.128.4.184-57788 (10.128.4.184:57788) with 64 cores
16/05/20 17:44:10 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/37 on hostPort 10.128.4.184:57788 with 64 cores, 100.0 GB RAM
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/36 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/37 is now RUNNING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/3 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/18 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/19 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/32 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/15 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/24 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/1 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/12 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/5 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/13 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/21 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/14 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/23 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/28 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/31 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/8 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/29 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/0 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/11 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/17 is now LOADING
16/05/20 17:44:10 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/7 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/27 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/16 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/4 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/22 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/20 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/26 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/6 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/10 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/9 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/33 is now LOADING
16/05/20 17:44:11 INFO EventLoggingListener: Logging events to file:/global/cscratch1/sd/qpzhang/spark/app-20160520174410-0000
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/34 is now LOADING
16/05/20 17:44:11 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/35 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/37 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/36 is now LOADING
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor added: app-20160520174410-0000/38 on worker-20160520174410-10.128.3.181-38131 (10.128.3.181:38131) with 64 cores
16/05/20 17:44:11 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160520174410-0000/38 on hostPort 10.128.3.181:38131 with 64 cores, 100.0 GB RAM
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/38 is now RUNNING
16/05/20 17:44:11 INFO MemoryStore: ensureFreeSpace(271176) called with curMem=0, maxMem=27783258439
16/05/20 17:44:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 264.8 KB, free 25.9 GB)
16/05/20 17:44:11 INFO MemoryStore: ensureFreeSpace(19788) called with curMem=271176, maxMem=27783258439
16/05/20 17:44:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.9 GB)
16/05/20 17:44:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.128.1.169:39209 (size: 19.3 KB, free: 25.9 GB)
16/05/20 17:44:11 INFO SparkContext: Created broadcast 0 from textFile at MLUtils.scala:75
16/05/20 17:44:11 INFO AppClient$ClientEndpoint: Executor updated: app-20160520174410-0000/38 is now LOADING
Exception in thread "main" org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/global/u2/q/qpzhang/Dropbox/Bitbucket/jgi-genelearn/scripts/Spark_Multiple_General/all.vect.order_training.svmlib.num.codon
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1942)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:985)
	at org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:109)
	at org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:138)
	at org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:163)
	at Simple$.main(SimpleApp.scala:39)
	at Simple.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/05/20 17:44:11 INFO SparkContext: Invoking stop() from shutdown hook
16/05/20 17:44:12 INFO SparkUI: Stopped Spark web UI at http://10.128.1.169:4040
16/05/20 17:44:12 INFO DAGScheduler: Stopping DAGScheduler
16/05/20 17:44:12 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/05/20 17:44:12 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/05/20 17:44:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/05/20 17:44:12 INFO MemoryStore: MemoryStore cleared
16/05/20 17:44:12 INFO BlockManager: BlockManager stopped
16/05/20 17:44:12 INFO BlockManagerMaster: BlockManagerMaster stopped
16/05/20 17:44:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/05/20 17:44:12 INFO SparkContext: Successfully stopped SparkContext
16/05/20 17:44:12 INFO ShutdownHookManager: Shutdown hook called
16/05/20 17:44:12 INFO ShutdownHookManager: Deleting directory /global/cscratch1/sd/qpzhang/spark-ef95e41c-c7a0-4187-bdb1-fd15448ff853
16/05/20 17:44:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-971500e0-0485-4262-a714-f4d0e6c60d9d
